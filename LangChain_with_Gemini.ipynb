{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mubashar-Bashir/Langchain_EcoSystem/blob/main/LangChain_with_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKzttDQNKgT4"
      },
      "source": [
        "# Google Gemini API with LangChain Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQL31LrCKgT6"
      },
      "source": [
        "## Environment Setup and Utility Functions\n",
        "**Install Required Package**\n",
        "**LangChain:** A framework for building applications using large language models, facilitating the creation of language model pipelines.\n",
        "\n",
        "**LangChain Google GenAI Integration:** An integration with Google's Generative AI tools, allowing for the use of advanced language models within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcTVPW0sKgT7",
        "outputId": "e527fbb3-5bc1-42bf-c3b0-0f7b2747172b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m481.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install the LangChain and LangChain's Google GenAI integration\n",
        "# `-q` keeps the output minimal.\n",
        "# `-U` ensures you are using the latest versions of the packages\n",
        "%pip install -q -U langchain\n",
        "%pip install -q -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper_function** to Format"
      ],
      "metadata": {
        "id": "Zv3qizIIM_gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the textwrap module for text formatting and indentation\n",
        "import textwrap\n",
        "\n",
        "# Import the Markdown display function from IPython to render text as Markdown in Jupyter Notebooks\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Define a function 'to_markdown' that converts a given text into Markdown format\n",
        "def to_markdown(text) -> Markdown:\n",
        "    # Replace bullet points (•) with Markdown-compatible bullet points (*)\n",
        "    text: str = text.replace(\"•\", \"  *\")\n",
        "\n",
        "    # Indent the entire text block with the Markdown blockquote symbol ('> ')\n",
        "    # The lambda function ensures every line is indented\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "tRrt7M7UNHwR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Access your API key in colab\n",
        "\n",
        "## Importing userdata from Google Colab to securely store and access API keys\n",
        "```from google.colab import userdata```\n",
        "     "
      ],
      "metadata": {
        "id": "UhCjBKmWNRON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "zJglMwrXNcP9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the API key, you can access it in Colab\n",
        "\n",
        "<li>Set the key in the GEMINI_API_KEY environment variable.\n",
        "<li>You can save this API key under any variable name you prefer.\n",
        "<li>Remember to enable access for the saved API key in Colab using the toggle button."
      ],
      "metadata": {
        "id": "NHVP4HRfN_9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after saving api key in env variables\n",
        "# get api key from env\n",
        "google_api_key = userdata.get('Gemini-key')"
      ],
      "metadata": {
        "id": "xm5hSFDtd5RY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Initializing LangChain with GEMINI for AI Chat Responses**"
      ],
      "metadata": {
        "id": "HM257Q56eAWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatGoogleGenerativeAI class from the langchain_google_genai module\n",
        "# this will be used for using langchain with gemni\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Import the AIMessage class currently will be used for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    api_key=google_api_key,     # Provide the Google API key for authentication\n",
        "    temperature=0.2,            # Set the randomness of the model's responses (0 = deterministic, 1 = very random)\n",
        ")"
      ],
      "metadata": {
        "id": "g9J9syeEeECP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Invoking LangChain Model with Prompt to Get Response**"
      ],
      "metadata": {
        "id": "FIA3AapCeP0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LangChain model with a prompt to generate a response\n",
        "ai_msg: AIMessage = llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "sPPR2unmeSun"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display complete response\n",
        "ai_msg"
      ],
      "metadata": {
        "id": "xULQWPAueXT4",
        "outputId": "1ecafab4-f841-4c40-bf03-bf8e6b38cb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"(Verse 1)\\nIn realms of code, where data flows,\\nA hero rose, LangChain, it shows.\\nWith chains of thought, it weaves a spell,\\nTo unlock knowledge, secrets to tell.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Verse 2)\\nWith LLM's power, it takes its stand,\\nTo understand, to learn, to command.\\nFrom text to code, it bridges the gap,\\nA symphony of knowledge, a digital map.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Verse 3)\\nIt gathers data, from near and far,\\nWith agents and tools, it shines like a star.\\nFrom APIs to databases, it knows the score,\\nUnlocking insights, forevermore.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Outro)\\nSo raise a glass, to LangChain's might,\\nA beacon of hope, in the digital night.\\nWith chains of thought, it sets us free,\\nTo explore the world, eternally. \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2ef94c1c-f988-4318-ac6c-3343a56a6c6e-0', usage_metadata={'input_tokens': 8, 'output_tokens': 331, 'total_tokens': 339})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get actual response\n",
        "ai_msg.content\n",
        ""
      ],
      "metadata": {
        "id": "XxOsyBk2eZ-r",
        "outputId": "d8089234-0c2e-4f71-e09b-4a2e8cc01491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"(Verse 1)\\nIn realms of code, where data flows,\\nA hero rose, LangChain, it shows.\\nWith chains of thought, it weaves a spell,\\nTo unlock knowledge, secrets to tell.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Verse 2)\\nWith LLM's power, it takes its stand,\\nTo understand, to learn, to command.\\nFrom text to code, it bridges the gap,\\nA symphony of knowledge, a digital map.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Verse 3)\\nIt gathers data, from near and far,\\nWith agents and tools, it shines like a star.\\nFrom APIs to databases, it knows the score,\\nUnlocking insights, forevermore.\\n\\n(Chorus)\\nOh, LangChain, LangChain, a tool so grand,\\nConnecting models, hand in hand.\\nFrom prompts to chains, it guides the way,\\nTo solve our problems, day by day.\\n\\n(Outro)\\nSo raise a glass, to LangChain's might,\\nA beacon of hope, in the digital night.\\nWith chains of thought, it sets us free,\\nTo explore the world, eternally. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the response with markdown\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "id": "kidyBWjQecy8",
        "outputId": "90095afc-4abf-4375-d63f-b2e11a800fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> (Verse 1)\n> In realms of code, where data flows,\n> A hero rose, LangChain, it shows.\n> With chains of thought, it weaves a spell,\n> To unlock knowledge, secrets to tell.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Verse 2)\n> With LLM's power, it takes its stand,\n> To understand, to learn, to command.\n> From text to code, it bridges the gap,\n> A symphony of knowledge, a digital map.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Verse 3)\n> It gathers data, from near and far,\n> With agents and tools, it shines like a star.\n> From APIs to databases, it knows the score,\n> Unlocking insights, forevermore.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Outro)\n> So raise a glass, to LangChain's might,\n> A beacon of hope, in the digital night.\n> With chains of thought, it sets us free,\n> To explore the world, eternally. \n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Invoking LangChain Model with Structured Messages for AI Responsesa**"
      ],
      "metadata": {
        "id": "WUPTjY3ZfVM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "message : list[Dict[str,str]] = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Which open source AI Model is best so far\"},\n",
        "]\n",
        "\n",
        "ai_msg = llm.invoke(message)"
      ],
      "metadata": {
        "id": "7yib-maOfaIO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "1ffOEmFjgIVD",
        "outputId": "54f435f5-c696-4eeb-ae84-9fe9a3fe19f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's impossible to definitively say which open-source AI model is \"best\" because the best model depends on your specific needs and use case. \n",
            "\n",
            "Here's a breakdown of some popular open-source AI models and their strengths:\n",
            "\n",
            "**Large Language Models (LLMs):**\n",
            "\n",
            "* **BLOOM:** A large language model with 176B parameters, trained on a massive dataset of text and code. It excels in various tasks like text generation, translation, and code completion.\n",
            "* **GPT-Neo:** A series of open-source models based on the GPT architecture, offering different sizes and capabilities. They are known for their strong performance in text generation and language understanding.\n",
            "* **BigScience:** A collaborative effort to develop large language models, including BLOOM. They offer a range of models with different sizes and training data.\n",
            "* **Flan-T5:** A model trained on a massive dataset of text and code, specifically designed for instruction following and question answering.\n",
            "\n",
            "**Image Generation Models:**\n",
            "\n",
            "* **Stable Diffusion:** A powerful text-to-image generation model that allows you to create stunning images from text prompts.\n",
            "* **DALL-E 2:** While not strictly open-source, DALL-E 2 has a public API and is considered a leading model for image generation.\n",
            "* **Imagen:** Another powerful text-to-image generation model from Google, known for its high-quality and realistic images.\n",
            "\n",
            "**Other Models:**\n",
            "\n",
            "* **Hugging Face Transformers:** A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\n",
            "* **OpenAI Whisper:** A powerful speech-to-text model that can transcribe audio and translate languages.\n",
            "\n",
            "**Factors to Consider When Choosing a Model:**\n",
            "\n",
            "* **Task:** What specific task do you want to perform?\n",
            "* **Data:** What type of data will you be using?\n",
            "* **Resources:** How much computational power and memory do you have available?\n",
            "* **Performance:** What level of accuracy and speed do you need?\n",
            "* **Ease of Use:** How easy is the model to use and integrate into your project?\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "* **For text generation and language understanding:** BLOOM, GPT-Neo, or Flan-T5\n",
            "* **For image generation:** Stable Diffusion, DALL-E 2, or Imagen\n",
            "* **For a wide range of tasks:** Hugging Face Transformers\n",
            "\n",
            "**Remember:** The best open-source AI model for you will depend on your specific needs and use case. It's important to research and experiment with different models to find the one that works best for you.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "id": "FU4azdxhgN9_",
        "outputId": "581b2366-fdef-4684-9c8c-a07c7e57c56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's impossible to definitively say which open-source AI model is \"best\" because the best model depends on your specific needs and use case. \n> \n> Here's a breakdown of some popular open-source AI models and their strengths:\n> \n> **Large Language Models (LLMs):**\n> \n> * **BLOOM:** A large language model with 176B parameters, trained on a massive dataset of text and code. It excels in various tasks like text generation, translation, and code completion.\n> * **GPT-Neo:** A series of open-source models based on the GPT architecture, offering different sizes and capabilities. They are known for their strong performance in text generation and language understanding.\n> * **BigScience:** A collaborative effort to develop large language models, including BLOOM. They offer a range of models with different sizes and training data.\n> * **Flan-T5:** A model trained on a massive dataset of text and code, specifically designed for instruction following and question answering.\n> \n> **Image Generation Models:**\n> \n> * **Stable Diffusion:** A powerful text-to-image generation model that allows you to create stunning images from text prompts.\n> * **DALL-E 2:** While not strictly open-source, DALL-E 2 has a public API and is considered a leading model for image generation.\n> * **Imagen:** Another powerful text-to-image generation model from Google, known for its high-quality and realistic images.\n> \n> **Other Models:**\n> \n> * **Hugging Face Transformers:** A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\n> * **OpenAI Whisper:** A powerful speech-to-text model that can transcribe audio and translate languages.\n> \n> **Factors to Consider When Choosing a Model:**\n> \n> * **Task:** What specific task do you want to perform?\n> * **Data:** What type of data will you be using?\n> * **Resources:** How much computational power and memory do you have available?\n> * **Performance:** What level of accuracy and speed do you need?\n> * **Ease of Use:** How easy is the model to use and integrate into your project?\n> \n> **Recommendations:**\n> \n> * **For text generation and language understanding:** BLOOM, GPT-Neo, or Flan-T5\n> * **For image generation:** Stable Diffusion, DALL-E 2, or Imagen\n> * **For a wide range of tasks:** Hugging Face Transformers\n> \n> **Remember:** The best open-source AI model for you will depend on your specific needs and use case. It's important to research and experiment with different models to find the one that works best for you.\n"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = llm.invoke(\"Sing a ballad of LangChain.\")"
      ],
      "metadata": {
        "id": "uGeRLlrchLHk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "id": "570LfYsahPhp",
        "outputId": "8cdba246-1cbe-4eb4-de5e-d028f59828bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> (Verse 1)\n> In realms of code, where data flows,\n> A hero rose, LangChain, it shows.\n> With chains of thought, it weaves a spell,\n> To unlock knowledge, secrets to tell.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Verse 2)\n> With LLM's power, it takes its stand,\n> To understand, to learn, to command.\n> From text to code, it bridges the gap,\n> A symphony of knowledge, a digital map.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Verse 3)\n> It gathers data, from near and far,\n> With agents and tools, it shines like a star.\n> From APIs to databases, it knows the score,\n> Unlocking insights, forevermore.\n> \n> (Chorus)\n> Oh, LangChain, LangChain, a tool so grand,\n> Connecting models, hand in hand.\n> From prompts to chains, it guides the way,\n> To solve our problems, day by day.\n> \n> (Outro)\n> So raise a glass, to LangChain's might,\n> A beacon of hope, in the digital night.\n> With chains of thought, it sets us free,\n> To explore the world, eternally. \n"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gemini vision model supports image inputs when providing a single chat message.**"
      ],
      "metadata": {
        "id": "r7pzgAuThtnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.api_core.exceptions import InternalServerError\n",
        "\n",
        "# Specify the model to use and Google API key (replace with your actual key)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        ")\n",
        "\n",
        "# Create the HumanMessage with a single image URL part\n",
        "message = HumanMessage(content= [{\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"}])\n",
        "\n",
        "try:\n",
        "    # Invoke the LLM with the message list (containing one message)\n",
        "    ai_response = llm.invoke([message])\n",
        "except InternalServerError as e:\n",
        "    print(f\"An internal server error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ebM1TWH1hyao",
        "outputId": "c8f57c25-ffd5-44e2-d997-fec12962c600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An internal server error occurred: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.api_core.exceptions import InternalServerError\n",
        "\n",
        "# Specify the model to use and Google API key (replace with your actual key)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=\"YOUR_GOOGLE_API_KEY\"\n",
        ")\n",
        "\n",
        "# Create the HumanMessage with a single image URL part\n",
        "message = HumanMessage(content=[{\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"}])\n",
        "\n",
        "try:\n",
        "    # Invoke the LLM with the message list (containing one message)\n",
        "    ai_response = llm.invoke([message])\n",
        "except InternalServerError as e:\n",
        "    print(f\"An internal server error occurred: {e}\")"
      ],
      "metadata": {
        "id": "4-V-Rmcrjrt2",
        "outputId": "c7f54350-2fed-42ec-d353-753bd8de77e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An internal server error occurred: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
            "Possible resolutions:\n",
            "- Ensure your Google Cloud project has the necessary API enabled and billing is set up correctly.\n",
            "- Check for any rate limits or quotas that may be affecting your requests.\n",
            "- Verify that the image URL provided is valid and accessible.\n",
            "- Try again later as the server might be experiencing temporary issues.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}